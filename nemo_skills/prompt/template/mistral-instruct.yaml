# Prompt specification for the original Mistral/Mixtral-instruct model, but skipping the system message
# these tokens are always used to construct a prompt like this
#
#   single-turn:
#     <text_begin><system_begin>{system}<system_end><user_begin>{user}<user_end><assistant_begin>{generation}
#   multi-turn:
#     <text_begin><system_begin>{system}<system_end><user_begin>{user1}<user_end><assistant_begin>{assistant1}<assistant_end>...
#     <user_begin>{userN}<user_end><assistant_begin>{generation}

text_begin: "<s>"

system_begin: " "
system_end: ""

user_begin: "[INST] "
user_end: " [/INST]"

assistant_begin: " "
assistant_end: "</s>"

stop_phrases: ["</s>"]

# used to execute code within these tags
code_begin: '<llm-code>\n'
code_end: '</llm-code>\n'
# used to extract the code output
code_output_begin: '<llm-code-output>\n'
code_output_end: '</llm-code-output>\n'
code_output_format: "qwen"