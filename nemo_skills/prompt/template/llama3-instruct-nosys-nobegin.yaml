# Prompt specification for the original Llama3-instruct model, but skipping the system message
# This apparanetly is quite important to reproduce the math eval numbers

# these tokens are always used to construct a prompt like this
#
#   single-turn:
#     <text_begin><system_begin>{system}<system_end><user_begin>{user}<user_end><assistant_begin>{generation}
#   multi-turn:
#     <text_begin><system_begin>{system}<system_end><user_begin>{user1}<user_end><assistant_begin>{assistant1}<assistant_end>...
#     <user_begin>{userN}<user_end><assistant_begin>{generation}

text_begin: ""

system_begin: ""
system_end: ""

user_begin: "<|start_header_id|>user<|end_header_id|>\n\n"
user_end: "<|eot_id|>"

assistant_begin: "<|start_header_id|>assistant<|end_header_id|>\n\n"
assistant_end: "<|eot_id|>"

stop_phrases: ["<|eot_id|>"]

# used to execute code within these tags
code_begin: '<|python_tag|>'
code_end: '<|eom_id|>'
# used to extract the code output
code_output_begin: '<|start_header_id|>ipython<|end_header_id|>'
code_output_end: '<|eot_id|><|start_header_id|>assistant<|end_header_id|>'