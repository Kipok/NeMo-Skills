# Prompt specification for the original Llama3-instruct model

# these tokens are always used to construct a prompt like this
#
#   single-turn:
#     <text_begin><system_begin>{system}<system_end><user_begin>{user}<user_end><assistant_begin>{generation}
#   multi-turn:
#     <text_begin><system_begin>{system}<system_end><user_begin>{user1}<user_end><assistant_begin>{assistant1}<assistant_end>...
#     <user_begin>{userN}<user_end><assistant_begin>{generation}

text_begin: ""

system_begin: "<|im_start|>system\n"
system_end: "<|im_end|>\n"

user_begin: "<|im_start|>user\n"
user_end: "<|im_end|>\n"

assistant_begin: "<|im_start|>assistant\n"
assistant_end: "<|im_end|>\n"

stop_phrases: ["<|im_end|>"]

# TODO: need to figure the right format for code execution
# used to execute code within these tags
code_begin: "```python\n"
code_end: "```\n"
# used to extract the code output
code_output_begin: "```output\n"
code_output_end: "```\n"
# used to post-process code output
# choices: llama, qwen
code_output_format: "qwen"